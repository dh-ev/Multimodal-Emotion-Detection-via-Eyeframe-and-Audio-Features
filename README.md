# Multimodal-Emotion-Detection-via-Eyeframe-and-Audio-Features

**Tech Stack : Python, TensorFlow, Pandas, LIME, Grad-CAM**

Emotion classification is a rapidly advancing field that seeks to decode human emotional expressions, both verbal and nonverbal, by leveraging statistical and machine learning techniques. With applications ranging from human-computer interaction to mental health diagnostics, accurately identifying emotions can significantly enhance the effectiveness of automated systems. The field has seen a notable shift from traditional statistical models, which often relied on feature extraction and simpler classifiers, to deep learning models that can automatically learn intricate features. Convolutional Neural Networks (CNNs) are especially well-suited for this task, given their ability to capture patterns in image and audio data. However, a major challenge remains in achieving robust generalization across different emotional expressions, especially when considering posed versus real emotions.
Previous works in emotion classification have demonstrated impressive results with uni-modal models that rely solely on either audio or visual data. However, these models often struggle to capture the full context of an emotion, particularly when handling complex emotions or distinguishing between posed and genuine expressions. While some multi-modal approaches have been introduced, they frequently lack the granularity needed to identify these nuanced distinctions, leading to reduced accuracy in real-world applications. Moreover, many studies have not incorporated datasets that include a wide range of both posed and real emotions, limiting the generalizability of their findings. This limitation highlights the need for a comprehensive, multi-modal approach that can differentiate between subtle emotional cues while maintaining robustness across diverse datasets.
To address these challenges, a multi-modal emotion classification model is developed that incorporates both audio and visual data to distinguish between 14 distinct emotions, including both posed and genuine expressions. By processing audio data through Mel-spectrograms and MFCC features and visual data through facial expression images, this approach combines features across modalities to better capture the complexity of emotional cues. This design allows for the identification of emotions from diverse datasets, including those with posed expressions, which are critical for applications in fields like mental health and customer service. The integration of separate CNN architectures for each modality enhances the modelâ€™s ability to accurately distinguish between real and posed emotions, providing a more nuanced understanding of each classification.

The structure of the model is designed to process audio and visual inputs separately, utilizing dedicated CNN architectures optimized for each modality. The audio data is transformed into Mel-spectrograms and MFCCs, which are fed into a CNN designed to capture time-frequency features. Meanwhile, the visual CNN processes 48x48 grayscale facial images, extracting spatial features indicative of different expressions. By combining outputs from these two parallel CNNs, the model performs final classification on the concatenated feature space, enabling it to jointly leverage audio and visual cues for enhanced emotional recognition. This multimodal architecture not only allows for greater accuracy but also supports the detection of subtle differences between real and posed emotions, addressing a key gap in previous emotion classification models.
